{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clarifai-grpc in /Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages (11.2.6)\n",
      "Requirement already satisfied: grpcio>=1.53.2 in /Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages (from clarifai-grpc) (1.70.0)\n",
      "Requirement already satisfied: protobuf>=3.20.3 in /Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages (from clarifai-grpc) (5.29.3)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.57.0 in /Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages (from clarifai-grpc) (1.65.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U voyageai pandas\n",
    "!pip install clarifai-grpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PAT: 54e90...d229 (masked for security)\n",
      "Using model: user_id=clarifai, app_id=main, model_id=apparel-classification-v2, version=651c5412d53c408fa3b4fe3dcc060be7\n",
      "Processing same_color directory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: same_color/yellow.jpg - coat (0.97); long-sleeve (0.94); 3/4 sleeve (0.67); chiffon (0.62); midi dress (0.54)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:07<00:14,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: same_color/green.jpg - coat (0.98); long-sleeve (0.85); 3/4 sleeve (0.76); midi dress (0.55); colorblock (0.50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:09<00:04,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: same_color/purple.jpg - coat (0.99); long-sleeve (0.92); v-neck (0.63); midi dress (0.62); 3/4 sleeve (0.54)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:12<00:00,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to clarifai_classification_results.json\n",
      "Processed results saved to processed_clarifai_classifications.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List, Optional\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import dotenv\n",
    "import traceback\n",
    "import base64\n",
    "from clarifai_grpc.channel.clarifai_channel import ClarifaiChannel\n",
    "from clarifai_grpc.grpc.api import service_pb2_grpc, service_pb2, resources_pb2\n",
    "from clarifai_grpc.grpc.api.status import status_code_pb2\n",
    "\n",
    "def classify_apparel_images(\n",
    "    directories: List[str],\n",
    "    max_images_per_dir: Dict[str, int],\n",
    "    pat: str,\n",
    "    user_id: str = \"clarifai\",\n",
    "    app_id: str = \"main\",\n",
    "    model_id: str = \"apparel-classification-v2\",\n",
    "    model_version_id: str = \"651c5412d53c408fa3b4fe3dcc060be7\",\n",
    "    max_concepts: int = 5,  # New parameter to specify maximum number of concepts to return\n",
    "    output_file: Optional[str] = \"classification_results.json\",\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Classify apparel images in specified directories using Clarifai API.\n",
    "\n",
    "    Args:\n",
    "        directories: List of directory names containing apparel images\n",
    "        max_images_per_dir: Dictionary mapping directory names to maximum number of images to process\n",
    "        pat: Clarifai Personal Access Token\n",
    "        user_id: Clarifai user ID\n",
    "        app_id: Clarifai app ID\n",
    "        model_id: Clarifai model ID\n",
    "        model_version_id: Optional model version ID (defaults to latest if None)\n",
    "        max_concepts: Maximum number of concepts to return per image\n",
    "        output_file: Optional file path to save results (None to skip saving)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing classification results for each image\n",
    "    \"\"\"\n",
    "    print(f\"Using model: user_id={user_id}, app_id={app_id}, model_id={model_id}, version={model_version_id}\")\n",
    "    \n",
    "    # Set up the gRPC client\n",
    "    channel = ClarifaiChannel.get_grpc_channel()\n",
    "    stub = service_pb2_grpc.V2Stub(channel)\n",
    "    metadata = (('authorization', 'Key ' + pat),)\n",
    "    user_data_object = resources_pb2.UserAppIDSet(user_id=user_id, app_id=app_id)\n",
    "\n",
    "    # Dictionary to store results\n",
    "    results = {}\n",
    "\n",
    "    # Process each directory\n",
    "    for directory in directories:\n",
    "        print(f\"Processing {directory} directory...\")\n",
    "\n",
    "        # Get image paths in the directory\n",
    "        image_pattern = os.path.join(directory, \"*.jpg\")\n",
    "        image_paths = glob.glob(image_pattern)\n",
    "\n",
    "        # Determine how many images to process\n",
    "        max_images = max_images_per_dir.get(directory, 10)  # Default to 10 if not specified\n",
    "        image_paths = image_paths[:max_images]\n",
    "\n",
    "        # Process each image\n",
    "        for image_path in tqdm(image_paths):\n",
    "            try:\n",
    "                # Read image file as bytes\n",
    "                with open(image_path, \"rb\") as f:\n",
    "                    image_bytes = f.read()\n",
    "                \n",
    "                # Create the request with file bytes instead of URL\n",
    "                post_model_outputs_response = stub.PostModelOutputs(\n",
    "                    service_pb2.PostModelOutputsRequest(\n",
    "                        user_app_id=user_data_object,\n",
    "                        model_id=model_id,\n",
    "                        version_id=model_version_id,\n",
    "                        inputs=[\n",
    "                            resources_pb2.Input(\n",
    "                                data=resources_pb2.Data(\n",
    "                                    image=resources_pb2.Image(\n",
    "                                        base64=image_bytes\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                        ]\n",
    "                    ),\n",
    "                    metadata=metadata\n",
    "                )\n",
    "\n",
    "                # Check for errors in the API response\n",
    "                if post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n",
    "                    error_msg = f\"API Error: {post_model_outputs_response.status.description}\"\n",
    "                    print(error_msg)\n",
    "                    results[image_path] = f\"Error: {error_msg}\"\n",
    "                    continue\n",
    "\n",
    "                # Get the output from the response\n",
    "                output = post_model_outputs_response.outputs[0]\n",
    "                \n",
    "                # Extract top concepts\n",
    "                if len(output.data.concepts) > 0:\n",
    "                    # Sort concepts by value (confidence score) in descending order\n",
    "                    concepts = sorted(output.data.concepts, key=lambda c: c.value, reverse=True)\n",
    "                    \n",
    "                    # Get up to max_concepts or all available if fewer\n",
    "                    top_concepts = concepts[:max_concepts]\n",
    "                    \n",
    "                    # Format the classifications\n",
    "                    classifications = [f\"{c.name} ({c.value:.2f})\" for c in top_concepts]\n",
    "                    \n",
    "                    # Join with semicolons for better readability\n",
    "                    classification_str = \"; \".join(classifications)\n",
    "                else:\n",
    "                    classification_str = \"No concepts found\"\n",
    "\n",
    "                # Store result\n",
    "                results[image_path] = classification_str\n",
    "                print(f\"Success: {image_path} - {classification_str}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Get detailed error information\n",
    "                error_details = traceback.format_exc()\n",
    "                print(f\"Error processing {image_path}:\")\n",
    "                print(error_details)\n",
    "                results[image_path] = f\"Error: {str(e)}\"\n",
    "\n",
    "            # Add a delay to avoid rate limiting\n",
    "            time.sleep(2.0)\n",
    "\n",
    "    # Save results to file if specified\n",
    "    if output_file:\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"Results saved to {output_file}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_classification_results(results_file: str) -> dict:\n",
    "    \"\"\"\n",
    "    Process classification results to extract only the item names without confidence scores.\n",
    "    \n",
    "    Args:\n",
    "        results_file: Path to the JSON file containing classification results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with image paths as keys and lists of item names as values\n",
    "    \"\"\"\n",
    "    # Load the results from the JSON file\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Dictionary to store processed results\n",
    "    processed_results = {}\n",
    "    \n",
    "    # Process each image result\n",
    "    for image_path, classifications in results.items():\n",
    "        # Skip error entries\n",
    "        if classifications.startswith(\"Error\") or classifications == \"No concepts found\":\n",
    "            processed_results[image_path] = []\n",
    "            continue\n",
    "        \n",
    "        # Split classifications by semicolon\n",
    "        items_with_scores = classifications.split('; ')\n",
    "        \n",
    "        # Extract just the item names (remove the confidence scores)\n",
    "        items = []\n",
    "        for item_with_score in items_with_scores:\n",
    "            # Extract the item name (everything before the opening parenthesis)\n",
    "            item_name = item_with_score.split(' (')[0]\n",
    "            items.append(item_name)\n",
    "        \n",
    "        # Store in the processed results dictionary\n",
    "        processed_results[image_path] = items\n",
    "    \n",
    "    # Save the processed results to a new JSON file\n",
    "    output_file = 'processed_clarifai_classifications.json'\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(processed_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Processed results saved to {output_file}\")\n",
    "    \n",
    "    return processed_results\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load environment variables from .env file\n",
    "    dotenv.load_dotenv()\n",
    "\n",
    "    # Get PAT from environment variable\n",
    "    pat = os.environ.get(\"PAT\")\n",
    "\n",
    "    if not pat:\n",
    "        print(\n",
    "            \"Error: PAT environment variable not found. Please set PAT in your .env file.\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    print(\n",
    "        f\"Using PAT: {pat[:5]}...{pat[-4:] if len(pat) > 8 else ''} (masked for security)\"\n",
    "    )\n",
    "\n",
    "    directories = [\"same_color\"]\n",
    "\n",
    "    # Process all images in the directory by setting a very high number\n",
    "    max_images_per_dir = {\n",
    "        \"same_color\": 1000  # This high number ensures we process all images\n",
    "    }\n",
    "\n",
    "    output_file = \"clarifai_classification_results.json\"\n",
    "    \n",
    "    # Classify images\n",
    "    results = classify_apparel_images(\n",
    "        directories=directories,\n",
    "        max_images_per_dir=max_images_per_dir,\n",
    "        pat=pat,\n",
    "        max_concepts=5,  # Get up to 5 classifications per image\n",
    "        output_file=output_file,\n",
    "    )\n",
    "\n",
    "    # Process the results to extract just the item names\n",
    "    processed_results = process_classification_results(output_file)\n",
    "    \n",
    "    return processed_results\n",
    "\n",
    "\n",
    "# Process existing results file without running classification again\n",
    "# processed_results = process_classification_results(\"clarifai_classification_results.json\")\n",
    "# processed_results\n",
    "\n",
    "results = main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing same_color/yellow.jpg - coat\n",
      "Processing same_color/yellow.jpg - long-sleeve\n",
      "Processing same_color/yellow.jpg - 3/4 sleeve\n",
      "Processing same_color/yellow.jpg - chiffon\n",
      "Processing same_color/yellow.jpg - midi dress\n",
      "Processing same_color/green.jpg - coat\n",
      "Processing same_color/green.jpg - long-sleeve\n",
      "Processing same_color/green.jpg - 3/4 sleeve\n",
      "Processing same_color/green.jpg - midi dress\n",
      "Processing same_color/green.jpg - colorblock\n",
      "Processing same_color/purple.jpg - coat\n",
      "Processing same_color/purple.jpg - long-sleeve\n",
      "Processing same_color/purple.jpg - v-neck\n",
      "Processing same_color/purple.jpg - midi dress\n",
      "Processing same_color/purple.jpg - 3/4 sleeve\n",
      "Analysis complete. Results saved to detailed_attribute_descriptions.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "from anthropic import Anthropic\n",
    "\n",
    "def initialize_anthropic_client():\n",
    "    api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"Error: ANTHROPIC_API_KEY not found in environment variables.\")\n",
    "        return None\n",
    "    return Anthropic(api_key=api_key)\n",
    "\n",
    "def analyze_attribute_with_claude(client, image_path, attribute):\n",
    "    \"\"\"Analyze a single attribute of a garment using Claude.\"\"\"\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image file not found at path: {image_path}\")\n",
    "        return {\"error\": \"Image file not found.\"}\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"source\": {\n",
    "                        \"type\": \"base64\",\n",
    "                        \"media_type\": \"image/jpeg\",\n",
    "                        \"data\": encoded_string\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"\"\"For this garment, focus only on the '{attribute}' aspect. \n",
    "                    Return a JSON object with: color, material, occasion, style, season, unique_feature, era, \n",
    "                    casual_or_relaxed (boolean), visual_aesthetic, hardware.\"\"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-7-sonnet-latest\",\n",
    "            max_tokens=800,\n",
    "            system=\"You are a fashion analyst focused on specific garment attributes.\",\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        response_text = response.content[0].text\n",
    "        json_start = response_text.find('{')\n",
    "        json_end = response_text.rfind('}') + 1\n",
    "        \n",
    "        if json_start >= 0 and json_end > json_start:\n",
    "            json_content = response_text[json_start:json_end]\n",
    "            try:\n",
    "                return json.loads(json_content)\n",
    "            except json.JSONDecodeError:\n",
    "                return {\"error\": \"Failed to parse JSON response\"}\n",
    "        else:\n",
    "            return {\"error\": \"No JSON in response\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error processing image: {e}\"}\n",
    "\n",
    "def describe_clothing_item_attributes(json_path, base_img_dir=\"\"):\n",
    "    client = initialize_anthropic_client()\n",
    "    if not client:\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r') as file:\n",
    "            processed_results = json.load(file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error loading JSON: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    detailed_descriptions = {}\n",
    "    \n",
    "    for image_path, attributes in processed_results.items():\n",
    "        full_image_path = os.path.join(base_img_dir, image_path) if base_img_dir else image_path\n",
    "        \n",
    "        # Create a list to store descriptions for each attribute\n",
    "        attribute_descriptions = []\n",
    "        \n",
    "        for attribute in attributes:\n",
    "            print(f\"Processing {image_path} - {attribute}\")\n",
    "            \n",
    "            # Get detailed analysis from Claude for this specific attribute\n",
    "            attribute_details = analyze_attribute_with_claude(client, full_image_path, attribute)\n",
    "            \n",
    "            # Add the attribute name to the details\n",
    "            attribute_details[\"attribute\"] = attribute\n",
    "            attribute_descriptions.append(attribute_details)\n",
    "        \n",
    "        # Store the list of attribute descriptions for this image\n",
    "        detailed_descriptions[image_path] = attribute_descriptions\n",
    "    \n",
    "    return detailed_descriptions\n",
    "\n",
    "def link_with_anthropic_descriptors():\n",
    "    json_path = \"processed_clarifai_classifications.json\"\n",
    "    base_img_dir = \"\"  # Set if your images are in a different directory\n",
    "    \n",
    "    results = describe_clothing_item_attributes(json_path, base_img_dir)\n",
    "    \n",
    "    with open(\"anthropic_clarifai_descriptions.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Analysis complete. Results saved to anthropic_clarifai_descriptions.json\")\n",
    "\n",
    "anthro_result = link_with_anthropic_descriptors()\n",
    "anthro_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: same_color/yellow.jpg\n",
      "Successfully embedded same_color/yellow.jpg\n",
      "Processing: same_color/green.jpg\n",
      "Successfully embedded same_color/green.jpg\n",
      "Processing: same_color/purple.jpg\n",
      "Successfully embedded same_color/purple.jpg\n"
     ]
    }
   ],
   "source": [
    "import voyageai\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (API key)\n",
    "load_dotenv(override=True)\n",
    "VOYAGE_API_KEY = os.getenv(\"VOYAGE_API_KEY\")\n",
    "\n",
    "# Initialize Voyage client with API key\n",
    "vo = voyageai.Client(api_key=VOYAGE_API_KEY)\n",
    "\n",
    "def load_attribute_descriptions(json_file_path):\n",
    "    \"\"\"Load the attribute descriptions from a JSON file.\"\"\"\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def prepare_text_for_embedding(attribute_list):\n",
    "    \"\"\"Convert attribute dictionary list to a single text string for embedding.\"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    for attr_dict in attribute_list:\n",
    "        attribute = attr_dict.get('attribute', 'unknown')\n",
    "        text_parts.append(f\"Attribute: {attribute}\")\n",
    "        \n",
    "        # Add all other fields\n",
    "        for key, value in attr_dict.items():\n",
    "            if key != 'attribute' and key != 'error':\n",
    "                text_parts.append(f\"{key}: {value}\")\n",
    "        \n",
    "        text_parts.append(\"---\")  # Separator between attributes\n",
    "    \n",
    "    return \" \".join(text_parts)\n",
    "\n",
    "def embed_fashion_descriptions(json_file_path, output_file=\"embeddings_same_color.csv\"):\n",
    "    \"\"\"Generate embeddings for each fashion item's full attribute description.\"\"\"\n",
    "    # Load the attribute descriptions\n",
    "    descriptions = load_attribute_descriptions(json_file_path)\n",
    "    \n",
    "    # Prepare DataFrame to store results\n",
    "    results = []\n",
    "    \n",
    "    # Process each image path\n",
    "    for image_path, attributes in descriptions.items():\n",
    "        print(f\"Processing: {image_path}\")\n",
    "        \n",
    "        # Convert the list of attribute dictionaries to a single text\n",
    "        full_description = prepare_text_for_embedding(attributes)\n",
    "        \n",
    "        try:\n",
    "            # Generate embedding for the entire dictionary as one text\n",
    "            embedding_result = vo.embed(\n",
    "                texts=[full_description],\n",
    "                model=\"voyage-3-large\",\n",
    "                input_type=\"document\",\n",
    "                output_dimension=256,\n",
    "                output_dtype=\"float\"\n",
    "            )\n",
    "            \n",
    "            # Store the result\n",
    "            results.append({\n",
    "                'image_path': image_path,\n",
    "                'embedding': embedding_result.embeddings[0],\n",
    "                'raw_attributes': attributes  # Store original attributes for reference\n",
    "            })\n",
    "            \n",
    "            print(f\"Successfully embedded {image_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error embedding {image_path}: {e}\")\n",
    "        \n",
    "        # Add a small delay to respect rate limits\n",
    "        time.sleep(3)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save to CSV (embeddings will be stored as strings)\n",
    "    df_to_save = df.copy()\n",
    "    df_to_save['embedding'] = df_to_save['embedding'].apply(lambda x: ','.join(map(str, x)))\n",
    "    df_to_save['raw_attributes'] = df_to_save['raw_attributes'].apply(json.dumps)\n",
    "    df_to_save.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Saved embeddings to {output_file}\")\n",
    "    return df\n",
    "\n",
    "# Function to search similar items using the embeddings\n",
    "def find_similar_fashion_items(df, query_image_path=None, query_text=None, top_n=5):\n",
    "    \"\"\"Find similar fashion items based on embeddings similarity.\"\"\"\n",
    "    if query_image_path is not None and query_image_path in df['image_path'].values:\n",
    "        # Get the embedding for the query image\n",
    "        query_embedding = df[df['image_path'] == query_image_path]['embedding'].iloc[0]\n",
    "    elif query_text is not None:\n",
    "        # Generate embedding for the query text\n",
    "        try:\n",
    "            query_result = vo.embed(\n",
    "                texts=[query_text],\n",
    "                model=\"voyage-3-large\",\n",
    "                input_type=\"document\",\n",
    "                output_dimension=256,\n",
    "                output_dtype=\"float\"\n",
    "            )\n",
    "            query_embedding = query_result.embeddings[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding for query text: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Either query_image_path or query_text must be provided\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate similarity scores\n",
    "    df['similarity'] = df['embedding'].apply(\n",
    "        lambda emb: sum(a*b for a, b in zip(emb, query_embedding)) / \n",
    "        (sum(a*a for a in emb)**0.5 * sum(b*b for b in query_embedding)**0.5)\n",
    "    )\n",
    "    \n",
    "    # Sort by similarity and return top matches\n",
    "    return df.sort_values('similarity', ascending=False).head(top_n)\n",
    "\n",
    "# Example usage in a Jupyter notebook\n",
    "# Run this in a cell\n",
    "\n",
    "# Load and embed fashion descriptions\n",
    "json_file_path = \"anthropic_same_color_descriptions.json\"\n",
    "embeddings_same_color_df = embed_fashion_descriptions(json_file_path, output_file=\"embeddings_same_color.csv\")\n",
    "embeddings_same_color_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/umap/umap_.py:2462: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/umap/umap_.py:2462: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n",
      "/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "float() argument must be a string or a real number, not 'dict'\n",
      "\n",
      "GraphQL request:18:7\n",
      "18 |       UMAPPoints(timeRange: $timeRange, minDist: $minDist, nNeighbors: $nNeighbo\n",
      "   |       ^\n",
      "   | rs, nSamples: $nSamples, minClusterSize: $minClusterSize, clusterMinSamples: $cl\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/graphql/execution/execute.py\", line 523, in execute_field\n",
      "    result = resolve_fn(source, info, **args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/strawberry/schema/schema_converter.py\", line 741, in _resolver\n",
      "    return _get_result_with_extensions(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/strawberry/schema/schema_converter.py\", line 728, in extension_resolver\n",
      "    return reduce(\n",
      "           ^^^^^^^\n",
      "  File \"/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/strawberry/schema/schema_converter.py\", line 723, in wrapped_get_result\n",
      "    return _get_result(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/strawberry/schema/schema_converter.py\", line 680, in _get_result\n",
      "    return field.get_result(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/strawberry/types/field.py\", line 223, in get_result\n",
      "    return self.base_resolver(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/strawberry/types/fields/resolver.py\", line 206, in __call__\n",
      "    return self.wrapped_func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/phoenix/server/api/types/EmbeddingDimension.py\", line 421, in UMAPPoints\n",
      "    ).generate(data, n_components=n_components)\n",
      "      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/phoenix/pointcloud/pointcloud.py\", line 64, in generate\n",
      "    projections = self.dimensionalityReducer.project(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/phoenix/pointcloud/projectors.py\", line 34, in project\n",
      "    return _center(UMAP(**config).fit_transform(mat))\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/umap/umap_.py\", line 2928, in fit_transform\n",
      "    self.fit(X, y, force_all_finite, **kwargs)\n",
      "  File \"/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/umap/umap_.py\", line 2372, in fit\n",
      "    X = check_array(\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1055, in check_array\n",
      "    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/sklearn/utils/_array_api.py\", line 839, in _asarray_with_order\n",
      "    array = numpy.asarray(array, order=order, dtype=dtype)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: float() argument must be a string or a real number, not 'dict'\n",
      "/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/lilysu/anaconda3/envs/py311/lib/python3.11/site-packages/umap/umap_.py:2462: UserWarning: n_neighbors is larger than the dataset size; truncating to X.shape[0] - 1\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Load the fashion embeddings data\n",
    "fashion_df = pd.read_csv('fashion_embeddings.csv')\n",
    "\n",
    "# Convert embedding strings to numpy arrays\n",
    "fashion_df['embedding'] = fashion_df['embedding'].apply(lambda x: np.array([float(val) for val in x.split(',')]))\n",
    "\n",
    "# Convert raw_attributes from string to dictionaries\n",
    "def parse_attributes(attr_str):\n",
    "    try:\n",
    "        # First, try parsing as a list of dictionaries\n",
    "        attributes = ast.literal_eval(attr_str)\n",
    "        # If it's a list, return the first item (or combine them if needed)\n",
    "        if isinstance(attributes, list):\n",
    "            return attributes[0]  # Alternatively, you could merge all dictionaries\n",
    "        return attributes\n",
    "    except:\n",
    "        # If parsing fails, return an empty dictionary\n",
    "        return {}\n",
    "\n",
    "fashion_df['parsed_attributes'] = fashion_df['raw_attributes'].apply(parse_attributes)\n",
    "\n",
    "# Extract specific attribute columns if needed\n",
    "for attr in ['color', 'material', 'style', 'occasion', 'season']:\n",
    "    fashion_df[attr] = fashion_df['parsed_attributes'].apply(lambda x: x.get(attr, '') if isinstance(x, dict) else '')\n",
    "\n",
    "# Define a Schema for Phoenix\n",
    "schema = px.Schema(\n",
    "    embedding_feature_column_names={\n",
    "        \"fashion_embedding\": px.EmbeddingColumnNames(\n",
    "            vector_column_name=\"embedding\",\n",
    "            link_to_data_column_name=\"image_path\",\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Wrap into Inferences object\n",
    "inferences = px.Inferences(\n",
    "    dataframe=fashion_df,\n",
    "    schema=schema,\n",
    "    name=\"Fashion Images\"  # Give your dataset a name\n",
    ")\n",
    "\n",
    "# Launch Phoenix!\n",
    "session = px.launch_app(primary=inferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
